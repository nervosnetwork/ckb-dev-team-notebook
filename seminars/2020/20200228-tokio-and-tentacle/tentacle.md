## Tentacle 回顾

这个项目开坑两年多了，中间经历了 Rust 各种功能稳定，包括异步的 n 种写法等问题。虽然项目名不见经传，但确实是公司几个项目的底层网络依赖，确实相对来说比较重要，也有必要让它的维护者尽可能多。

### 来源

tentacle 从 libp2p 简化而来，很多暂时看不到收益的东西被直接删除了，能依据协议实现的地方尽量依据协议来，其他地方尽可能简单实现，初衷是为了尽快稳定可用，从开发到生产使用实际上并没有太多的时间。

网络其实算是计算机世界里面最无聊、坑也最多的地方了，从所谓的 C10 问题到异步编程到所谓的多路复用都是使网络变得愈加复杂的地方，复杂带来的永远是各种奇怪的问题，而一个分布式项目，起点就是一个相对可靠的网络环境。

### 难点

tentacle 库的难点不止来源于网络本身，它还来源于 Rust 语言，比如：

- 异步 runtime 的统一
- Future 本身带来的问题
- Rust 语言本身进度问题
- 长连接场景下的状态管理问题

网络本身的问题：

- 如何尽可能发现节点，包含 NAT 穿透
- 保证网络正常工作
- 支持广播等功能

从另一个角度看，对网络库本身的要求很低，只要能保证行为正确，可以正常通信就可以了，但在实现上，会有各种奇怪的暗坑，这也是工程上的通用痛点之一。

### 实现

从某种意义上来说，tentacle 的实现是极其简单且死板的东西，因为它并不存在任何意义上的创新，也不存在任何黑魔法，只是在异步的世界里面将需要的东西按部就班实现完成，大体区分模块大致是：

- runtime 封装：让 tentacle 可以跑在任意 runtime 上，包括 wasm
- 协议封装：让 tentacle 支持 TCP 、WebSocket 及其他可能的底层协议
- secio 加密库：非对称加密握手，aead 通信的实现
- yamux：实现多路复用
- tentacle：将多路复用封装成协议，提供用户接口

将每一个模块拆开看其实并不复杂，甚至极其简单，但合并的过程并不是很顺滑，甚至令人恼火，然而，网络世界就是这样的套娃存在。

#### 协议封装（transport）

我们先从最底层，协议封装来看，TCP 是流式协议，WebSocket 是流式协议但 http 是基于消息的而它的底层 TCP 又是流式协议，在封装的过程中，我们需要将所有底层协议封装成流式协议接口，即提供类似这样的接口：

```rust
fn read(&mut self, buf: &mut [u8]) -> Result<usize, io::Error>;
fn write(&mut self, buf: &[u8]) -> Result<usize, io::Error>;
fn flush(&mut self) -> Result<(), io::Error>;
```

如果大家都是流式协议，那么封装成这样基本上没什么性能损失，相当于多了一层函数调用，如果编译器优化得好，这层调用可能被 inline 掉，也就相当于没有，但基于消息的协议封装成流式接口，那么不可避免得需要一些额外的 copy 和封装拆解消息包的过程，这里就是一点细微损失。

当然，这也符合计算机世界的规律，每多一层封装，必然会引起一定的损失，要想提高性能，拆封装层数就可以达到要求。

#### secio

这是一个非常简单的加密库，如果从最抽象的角度来看的话，它实际上只有两个功能：

- 握手阶段，协商对称私钥，并给出包装好的流式接口/对方的公钥
- 通信阶段，aead 加解密通信

从提供的接口来看，它吞下了一个流式 handle，并吐出了一个带加密的流式接口，但它内部其实是基于消息的，也就意味着，任何消息的写入，都必须经过完整的加密过程，然后组装成 `header|data` 的方式送给底层流式接口，消息的接收就是逆过程，也就是说，这里做了一次转换。

这里会碰到一个 Rust 异步的经典问题：能否用 `async fn` 实现 `poll fn`？如果实现会碰到什么坑？（有 ppt）

`poll_write` 的语义原本应该与 `write` 一致，即将数据写入底层 io，并应该是已经或者正在发送了，唯一的区别是，`write` 在 io blocking 的时候是卡住当前线程直到写完为止（先忽略其他错误），而 `poll_write` 则是在 blocking 的时候立刻返回 `pending` 。将基于消息的协议封装成基于流的接口的时候，就会出现以下情况，在包装流数据成为自身协议包数据的时候，数据已经完全被修改掉了，对于上层的响应只能是，完全写入或者完全没有写入，并且还需要尝试 flush，这是因为使用的 Framed 结构如果不 flush 就只是写在自身的 buffer 中，并没有尝试刷入底层 io，于是 secio 的内部实现就变成了下面这样：

```rust
fn poll_write(
    mut self: Pin<&mut Self>,
    cx: &mut Context<'_>,
    buf: &[u8],
) -> Poll<io::Result<usize>> {
    match self.socket.poll_ready_unpin(cx) {
        Poll::Ready(Ok(_)) => {
            let frame = self.encode_buffer(buf);
            self.socket.start_send_unpin(frame)?;
            let _ignore = self.socket.poll_flush_unpin(cx)?;
            Poll::Ready(Ok(buf.len()))
        }
        Poll::Pending => Poll::Pending,
        Poll::Ready(Err(e)) => Poll::Ready(Err(e)),
    }
}
```

#### yamux

yamux 是一个非常简单的多路复用协议，基于消息，但对外提供流式接口。

消息格式很简单：

- header：表示当前消息的类型，归属 id，长度，flag等等
- body: optional，真实的通信数据会放在这里

它还有类似于 TCP 的消息窗口管理机制，即一个数据消息会附带一个 window update 消息，用来控制数据拥塞问题，各类机制都相对完善。

实现它也会碰到问题，在 Rust 当前的异步生态里面，是缺失 async drop 这样的东西的，就是说，在异步环境下要保证异步回收资源目前是相对比较困难的，尤其是加上 async runtime 的问题之后，显得由为头疼。

我们先来理解 yamux 实现，它实际上是一个一对多的关系映射，通过 header 上的 id 标识决定消息去往哪个 substream，从而实现多路复用，那么必然存在一个问题，主流必须保存所有已经开启的自流的信息，只有当自流关闭的时候，通知主流，主流才会释放对应的资源，而关闭，对应在 rust 里面最好的实现就是 drop 的时候检查资源释放，且必须保证释放的信号一定要通知到位。

在 Rust 里面，有完全所有权的两个数据结构想要通知对方做一件事情，有两种办法：

- 用 Arc + mutex 将状态共享，同时 session 做一个类似 GC 的功能去检查这个状态（参考实现是 parity 的实现）
- 用 Channel 通知（我们的实现）

#### tentacle

这里汇聚了很多实现，包括：

- 对用户的 API 接口
- 对 transport 的封装
- 对 runtime 的封装
- 实现带优先队列的 channel
- 一些非常 trick 的实现

这里还有一些悬而未决的思考：

1. tentacle 是不是做的事情过多了，但做的事情太少会导致框架退化为库，这是一件令人头疼的事情
2. 用户接口是不是应该换风格，一直在怀疑自己的品味问题
3. 到底怎么样才能让它既能简单实现又能支持复杂的应用

在 tentacle 实现里面，最多的场景就是消息分发和聚合，异步的写法不同会导致一些坑点，当然，这个坑在理想环境中是不会出现的，只有各种极端环境才需要考虑这个问题：

##### 建议在合适的地方使用 async，在不合适的地方使用状态机

典型场景：消息分发

```rust
for sender in senders {
	sender.send(data).await
}
```

如果其中一个 sender pending，将导致后续所有 sender 等待，因为这是异步场景下的串行执行。

tentacle 目前内部使用的是 poll + buffer + try_send，会避免这样的问题，这也是为什么 tentacle 异步使用混合模式的原因之一。

##### 多个 Future 手动合并

最近发现的一个 bug，导致消息可能被遗漏在框架的多级缓存中。

核心问题在于，简单的 Future 只有一个任务，当它被 wake 的时候，只需要尝试执行这个任务就可以了；多个 Future 组合在一起的时候，当它被唤醒，需要人工保证可能的 pending 都至少执行一次，否则就相当于其中一个 pending 任务丢失了信号，无法被唤醒。这是在手动实现 Future 的时候需要格外小心的地方。 

#### 发现协议与 NAT

ckb 的发现模型参考的是 bitcoin 的尽力发现模型，与 ipfs 实时构建 kad 网络拓扑不一样的地方在于，ckb 只关心直连的部分，其余部分几乎是没有感知的，所以重点在于如何尽可能连上/被连上更多的节点。

从单机世界进入分布式世界之后，服务间的发现问题一直是必须要解决的问题，现在市面上有的方案：

- MDNS 组播：同网段及其子网
- Service mesh：全服务公网或者说可达，加上类似 etcd 的全局服务注册机制
- Proxy or Relay：需要已知的公网服务器提供对私有网络的代理转发功能，类似 stun/upnp
- IP 隧道：在 P2P 应用中，因存在各种类型的节点，保证连接不断的情况下，公网节点可以尝试将私有网络的观察 IP 传播到全网，通过已知的 IP 隧道尝试连接


更多参考文章可以找 blog 中实现过程中的输出文章及 [PPT](https://docs.google.com/presentation/d/1bIrOMSXuJ6bWQH26I6AZAy6RFgJE9i8IHrfIP2tyE9s/edit?usp=sharing)
